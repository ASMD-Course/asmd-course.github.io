# Lab03

## Objectives

- Understand the potential of LLMs in software development
- Experiment with LLMs in the context of software testing
- Investigate the effectiveness of LLMs in a TDD scenario
- Use of LLMs programmatically to generate test cases

## Tasks
1. **CODE GENERATION**
   - **Goal**: evaluate the effectiveness of various LLMs in generating solutions for OOP exam from previous years.
   - **Task**: utilize multiple LLMs (such as ChatGPT, GitHub Copilot, and Codex) to attempt solving past OOP exam questions. Assess which models deliver accurate solutions and document
   any modifications you apply to enhance the modelsâ€™ responses.
   - **Additional Task**: experiment with different prompting strategies (e.g., zero-shot, few-shot) to understand their impact on solutions.

2. **TESTING**
   - **Goal**: analyze the quality of test cases generated by LLMs for existing solutions to OOP exams
   - **Task**: remove the existing tests and use LLMs to regenerate test cases. Evaluate whether the newly generated tests 
   are comprehensive and retain the characteristics of the original tests. Try to guide the LLMs to generate tests that are more effective and efficient.

3. **TDD**
    - Goal: investigate the effectiveness of Copilot in a Test-Driven Development (TDD) scenario.
    - Task: apply TDD principles to solve a given a PPS exercise (https://github.com/unibo-pps/pps-23-24-lab01b, exercise 2) using an LLM (you can use both Java or Scala to
    - implement it). Assess whether incremental steps and test-first development aid the LLM in generating correct code solutions.


### Code Generation
